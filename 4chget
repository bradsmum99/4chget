#!/bin/bash
#
# 4chget - a simple script for downloading 4chan images
#            by Greg Naughton, (c) 2011
#

if [ ! $1 ]; then
    echo usage: $(basename $0)" [-A] <4chan url> <dir to save img dump>"
    echo "  1st arg: 4chan page URL"
    echo "  2nd arg (optional): directory to dump images to"
    echo "   * if no dir provided, images will be dumped into current path"
    echo "   * if -A option is used, the entire page will be archived"
    exit
fi

ARG1=$1; ARG2=$2
if [ $1 == "-A" ]; then ARCHIVE=1; ARG1=$2; ARG2=$3; fi

OUTDIR=$(pwd)
if [ $ARG2 ]; then
    OUTDIR=$ARG2
    if [ ! -d $OUTDIR ]; then
        echo directory: \"$OUTDIR\" doesn\'t exist. making it.
        mkdir $OUTDIR
    fi
    cd $OUTDIR
else
    echo warning: images will be saved to \"$OUTDIR\" "Proceed [y/N]? "
    read ans;
    if [ $ans != 'y' ] && [ $ans != 'Y' ]; then
        echo aborting...
        exit;
    fi 
fi
echo saving images to \"$OUTDIR\"

COUNT=0
SKIP=0

# default to wget if it's available
if which wget >/dev/null; then HAVE_WGET=1; fi


PAGE=`curl $ARG1` 
if echo "$PAGE" | grep -q "404 - Not Found"; then
    echo page 404\'d, no longer exists
    exit;
fi

URLS=`echo $PAGE | tr ' ' '\n' | grep -E http:\/\/images.4chan.org\/[a-z]+\/src\/[0-9]*\.[jpg][pni][gf] | sed -e 's/href="//g' -e 's/"//g' | uniq`

#
# ARCHIVE option downloads html and thumbnails as well
#
if [ $ARCHIVE ]; then
    echo Archiving Page with Thumbnails
    PAGE_NUM=`echo $ARG1 | sed -E 's_http:\/\/boards.4chan.org\/[a-z]+\/res\/__'`
    TDIR=thumbs_${PAGE_NUM}
    THUMB_URLS=`echo $PAGE | tr ' ' '\n' | grep -E thumbs.4chan.org\/[a-z]+\/thumb\/[0-9]*s\.[jpg][pni][gf] | sed -e 's/src=//g' | uniq`
    PAGE=`echo $PAGE | sed -E -e 's_http:\/\/images.4chan.org\/[a-z]+\/src\/__g' -e "s^http:\/\/[0-9]+.thumbs.4chan.org\/[a-z]+\/thumb^${TDIR}^g"`

    OUT=`if [ $ARG2 ]; then echo $(basename $ARG2)--$PAGE_NUM; else echo $PAGE_NUM; fi`
    echo $PAGE > $OUT.html

    #get thumbs, to a directory named 'thumbs_page#'
    if [ ! -e $TDIR ]; then mkdir ${TDIR}; fi

    for thurl in $THUMB_URLS; do 
        tname=`echo $thurl | sed -E 's_http:\/\/[0-9]+.thumbs.4chan.org\/[a-z]+\/thumb\/__'`
        if [ ! -e ${TDIR}/${tname} ]; then
            curl $thurl -o ${TDIR}/${tname}
        fi
    done
fi

for line in ${URLS};
do 
    pic=`echo $line | sed -r 's/http:\/\/images.4chan.org\/[a-z]+\/src\///g'`
    if [ ! -e $pic ]; then
        let COUNT="${COUNT}+1"
        echo $COUNT
        if [ $HAVE_WGET ]; then
            wget -c $line
        else
            curl $line -o $pic
        fi
    else
        let SKIP="${SKIP}+1"
        echo skipping "${pic} (#${SKIP})", already have it 
    fi
done 

if [ $SKIP -gt 0 ]; then
    echo -n "skipped getting $SKIP files, "
fi

if [ $COUNT -gt 0 ]; then
    echo got $COUNT new images.
else
    echo there were no new images
fi
